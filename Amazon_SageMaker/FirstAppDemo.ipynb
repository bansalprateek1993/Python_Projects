{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71d6854",
   "metadata": {},
   "source": [
    "## Steps to be followed:\n",
    "    1.IMporting necessary libraries.\n",
    "    2. Creating S3 bucket.\n",
    "    3. Mapping train and test data in S3.\n",
    "    4. Mapping the path of the models in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6385688",
   "metadata": {},
   "source": [
    "## Using some built in algo, it is present in sagemaker like XGboost. We will be downloading image container which has already installed in get_image_uri\n",
    "\n",
    "## boto3 - python from local environment we can read S3 buckets if its public.\n",
    "\n",
    "## S3 sessions - to use instance we need to create session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bb0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f414da8",
   "metadata": {},
   "source": [
    "## Can create a bucket based on different regions, so thats why checking the region first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef11cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'bankappdemo'\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f14c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537efd40",
   "metadata": {},
   "source": [
    "### to save a model into s3 bucket and versioning purpose, we can get the model name. As of now we are using the xgboost-as-a-built-in-algo model. So we can use that directly. \n",
    "### Once we train my model, if will be stored in the below output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54cb697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://bankappdemo/xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d9dd2",
   "metadata": {},
   "source": [
    "## Downloading dataset into amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d0ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02f795",
   "metadata": {},
   "source": [
    "### Dividing Train-test data - we can also use this data to store it into our S3 bucket so that it can be used for later purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e33786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93d0f5",
   "metadata": {},
   "source": [
    "### Amazon sagemaker requires reformatting. Dependent features should be in first column\n",
    "### Creating train.csv\n",
    "### Using boto3 session to access S3 and putting the train data into S3 bucket\n",
    "### Data path should be given from S3 bucket. For that we have to create a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67e66533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab563e",
   "metadata": {},
   "source": [
    "### Doing the same for test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c89f71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "s3_input_test = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/test'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ac8d5",
   "metadata": {},
   "source": [
    "### Build models Xgboost - In build algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d03fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b33e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max-depth\" : \"5\",\n",
    "    \"eta\" : \"0.2\",\n",
    "    \"gamma\" : \"4\",\n",
    "    \"min-child-weight\" : \"6\",\n",
    "    \"subsample\" : \"0.7\",\n",
    "    \"objective\" : \"binary-logistic\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989acf86",
   "metadata": {},
   "source": [
    "## constructing sagemaker estimator that calls container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a459343",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(container,sagemaker.get_execution_role(), instance_count=1, instance_type='ml.m4.xlarge',output_path=output_Path,sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee391b",
   "metadata": {},
   "source": [
    "### Running training by calling fit from estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1185dc",
   "metadata": {},
   "source": [
    "## Deploying ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9ff8f",
   "metadata": {},
   "source": [
    "### Prediction of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac542356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fc9cf",
   "metadata": {},
   "source": [
    "## Deleting the end points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49afdcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': '6SE6G38M5GZDB08D',\n",
       "   'HostId': 'rz7ib2vdT4w8VJZag7RQD+ON6jpG+dorf+0AnUSFC3GVwLjprCrAYz3EwJcSCZltyv4F14qoBYI=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'rz7ib2vdT4w8VJZag7RQD+ON6jpG+dorf+0AnUSFC3GVwLjprCrAYz3EwJcSCZltyv4F14qoBYI=',\n",
       "    'x-amz-request-id': '6SE6G38M5GZDB08D',\n",
       "    'date': 'Sun, 05 Sep 2021 18:06:02 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'xgboost-as-a-built-in-algo/train/train.csv'},\n",
       "   {'Key': 'xgboost-as-a-built-in-algo/test/test.csv'}]}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d72f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
